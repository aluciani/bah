#include "iostream.bah"

#include "./globals.bah"

//tokens types
#define tokenType char

const TOKEN_NO_TYPE =      <tokenType>-1
const TOKEN_TYPE_INT =     <tokenType>0
const TOKEN_TYPE_FLOAT =   <tokenType>1
const TOKEN_TYPE_VAR =     <tokenType>2
const TOKEN_TYPE_ENCL =    <tokenType>3
const TOKEN_TYPE_SEP =     <tokenType>4
const TOKEN_TYPE_STR =     <tokenType>5
const TOKEN_TYPE_KEYWORD = <tokenType>6
const TOKEN_TYPE_CHAR =    <tokenType>7
const TOKEN_TYPE_BOOL =    <tokenType>8
const TOKEN_TYPE_SYNTAX =  <tokenType>10
const TOKEN_TYPE_FUNC =    <tokenType>11
const TOKEN_TYPE_CAST =    <tokenType>12

//a token (the atomic unit for describing the inputed Bah file)
struct Tok {
    cont: cpstring = ""
    ogCont: cpstring = ""
    type: tokenType = TOKEN_NO_TYPE
    pos: uint32 = 0
    line: uint32 = 1
    begLine: uint32 = 1
    bahType: cpstring = "" //shortcut for getting the type of a token
    isValue: bool = false
    isFunc: bool = false
    isOper: bool = false
    isEqual: bool = false
    pass: bool = false
    bahRefType: char
    bahRef: ptr
    processedStr: bool = false
    processedPtr: bool = false

    //optimizations
    isNotExpsvOper: bool = false
    parent: variable*
    isExpensive: bool

    getRefVar() ptr {
        if this.bahRefType != 'v' {
            return null
        }
        return this.bahRef
    }
    
    getRefFn() ptr {
        if this.bahRefType != 'f' {
            return null
        }
        return this.bahRef
    }

    setRefVar(v ptr) {
        this.bahRef = v
        this.bahRefType = 'v'
    }

    setRefFn(f ptr) {
        this.bahRef = f
        this.bahRefType = 'f'
    }

}


enclavers = []char{'(', ')', '{', '}', '[', ']'}
syntaxes = []char{'!', '=', '|', <char>38, '%', '+', '-', '*', '/', ',', '<', '>', ':', <char>59, '^'}
                                    //&                                                    //;
keywords = []cpstring{"if", "else", "for", "return", "new", "break", "continue", "struct", "const", "extend", "function", "async", "in", "chan", "map", "buffer", "let", "then", "default", "switch", "case", "while", "typedef"}
                    //  x      x      x        x       x       x          x          x        x                    x         x       x      x      x       x         x      x        -          -        -       -          -
                    // x: done
                    //  : not done
                    // -: not planned

//Generates a token from all of its properties.
makeToken(pos int, lineNb int, cont []char, type tokenType) Tok {
    t = Tok{}
    t.cont = arrToStr(cont)
    t.ogCont = t.cont
    clear(cont)

    t.pos = pos
    t.line = lineNb
    t.type = type
    
    if type == TOKEN_TYPE_VAR && t.cont in keywords {
        t.type = TOKEN_TYPE_KEYWORD
    } else if type == TOKEN_TYPE_INT || type == TOKEN_TYPE_STR || type == TOKEN_TYPE_FLOAT || type == TOKEN_TYPE_VAR || type == TOKEN_TYPE_BOOL || type == TOKEN_TYPE_CHAR {
        t.isValue = true
    }
    return t
}

//If current char is a '-' and next char is a number, returns true.
isMinus(c char, nc char) bool {
    return c == '-' && isNumber(nc)
}

//We never want to have to throw this error as the file is not yet parsed, we have very little information.
lexerErr(line int, pos int, msg cpstring) {
    lineStr = intToStr(line)
    posStr = intToStr(pos)
    println("\e[1;31m[LEXER-ERROR]\e[0m "+compilerState.currentFile+":"+lineStr+":"+posStr+"\n\e[0m\n"+msg)
    exit(1)
}

//Takes a file as input, outputs an array of tokens.
lexer(s cpstring, codeLength uint) []Tok {
    dur = getTimeUnix()
    tokens = []Tok

    totalSize += codeLength

    memory = []char

    lineNb = 1 //current line number (in source file)

    i=0; for i < codeLength, i++ {
        c = s[i]
        nc char = <char>0
        if i+1 < codeLength {
            nc = s[i+1]
        }
        //testing for comments
        if c == '/' {
            nc = s[i+1]
            if nc == '/' {
                //ignore the content untill we hit a new line
                for i < codeLength, i++ {
                    c = s[i]
                    if c == <char>10 {
                        break
                    }
                }
                //file end
                if i == codeLength {
                    break
                }
            }
        }

        //we have it the new line character
        if c == <char>10 {
            lineNb++
        }

        //token is a string
        if c == '"' {
            pos = i
            begLine = lineNb
            memory[0] = c
            i = i + 1
            for i < codeLength, i++ {
                c = s[i]
                if c == <char>92 {
                    memory[len(memory)] = <char>92
                    memory[len(memory)] = s[i+1]
                    if s[i+1] == <char>10 {
                        lineNb++
                    }
                    i++
                    continue
                }
                
                //string ending
                if c == '"' {
                    memory[len(memory)] = c
                    break
                }
                //escaping line returns
                if c == <char>10 {
                    memory[len(memory)] = <char>92
                    memory[len(memory)] = 'n'
                    lineNb++
                    continue
                }
                memory[len(memory)] = c
            }
            tokens[len(tokens)] = makeToken(pos, lineNb, memory, TOKEN_TYPE_STR)
            lt = tokens[len(tokens) - 1]
            lt.begLine = begLine
            tokens[len(tokens) - 1] = lt
        } else if isNumber(c) || isMinus(c, nc) { //token is a number
            memory[0] = c
            pos = i
            i = i + 1
            currentType = TOKEN_TYPE_INT
            isHex = false
            for i < codeLength, i++ {
                c = s[i]
                if c == <char>46 {
                    currentType = TOKEN_TYPE_FLOAT
                } else if isNumber(c) == false {
                    if isHex == false {
                        if c == 'x' {
                            isHex = true
                        } else {
                            break
                        }
                    } else {
                        if isUpper(c) {
                            c += <char>32
                        }
                        if c < 'a' || c > 'f' {
                            break
                        }
                    }
                    if isHex == false {
                        break
                    }
                }
                memory[len(memory)] = c
            }
            i--
            tokens[len(tokens)] = makeToken(pos, lineNb, memory, currentType)
        } else if c == <char>39 { //is a character
            i++
            n = s[i]
            toInt = intToStr(<int>nc)
            memory = strToArr(toInt)
            i++
            c = s[i]
            if c != <char>39 {
                lexerErr(lineNb, i, "Missing closing tag in char declaration.")
            }
            tokens[len(tokens)] = makeToken(i-1, lineNb, memory, TOKEN_TYPE_CHAR)
        } else if c == <char>35 { //is a hash keyword
            pos = i
            memory[0] = c
            i++
            for i < codeLength, i++ {
                c = s[i]
                if isAlphaNumeric(c) == false {
                    break
                }
                memory[len(memory)] = c
            }
            i--
            tokens[len(tokens)] = makeToken(pos, lineNb, memory, TOKEN_TYPE_KEYWORD)
        } else if c == '.' { //token is a separator
            memory[0] = c
            tokens[len(tokens)] = makeToken(i, lineNb, memory, TOKEN_TYPE_SEP)
        } else if isAlphaNumeric(c) || c == '_' { //token is a var / keyword
            memory[0] = c
            pos = i
            i++
            //get full token
            for i < codeLength, i++ {
                c = s[i]
                if isAlphaNumeric(c) == false {
                    if c != '_' {
                        if c == '>' {
                            lc = memory[len(memory)-1]
                            if lc == '-' {
                                i--
                                break
                            }
                        }
                        break
                    }
                }

                memory[len(memory)] = c
            }
            i--
            currentType = TOKEN_TYPE_VAR
            
            tokens[len(tokens)] = makeToken(pos, lineNb, memory, currentType)
        } else if c in syntaxes { //token is a syntax element
            

            if c == '<' {
                pos = i
                isCast = false
                memory[0] = c
                i++
                for i < codeLength, i++ {
                    c = s[i]
                    if isSpace(c) {
                        continue
                    }
                    if c == '>' {
                        isCast = true
                        memory[len(memory)] = c
                        break
                    }
                    if isAlphaNumeric(c) == false && c != '*' && c != ':' && c != '_' && c != '[' && c != ']' {
                        isFnType = (c == '(' || c == ')') && string(arrToStr(memory)).hasPrefix("<function")
                        if isFnType == false {
                            break
                        }
                    }
                    memory[len(memory)] = c
                }
                if isCast == true {
                    tokens[len(tokens)] = makeToken(pos, lineNb, memory, TOKEN_TYPE_CAST)
                    continue
                }
                i = pos
                c = '<'
                clear(memory)
            }
            
            memory[0] = c
            pos = i
            i++
            fc = c
            for i < codeLength, i++ {
                c = s[i]
                if c in syntaxes == false {
                    break
                }
                //for <=, >=, ==, !=, +=, -=, &&, ||, <<
                if fc == '<' {
                    if c != '-' && c != '=' && c != '<' {
                        break
                    }
                } else if c == '|' {
                    if fc != c {
                        break
                    }
                } else if c == '&' {
                    if fc != c {
                        break
                    }
                } else if c != '=' {
                    if c != '>' {
                        break
                    }
                }
                memory[len(memory)] = c
            }
            i--
            tokens[len(tokens)] = makeToken(pos, lineNb, memory, TOKEN_TYPE_SYNTAX)
        } else if c in enclavers { //token is enclaver
            memory[0] = c
            tokens[len(tokens)] = makeToken(i, lineNb, memory, TOKEN_TYPE_ENCL)
        }

    }
    totalLines = totalLines + lineNb - 1
    dur = getTimeUnix() - dur
    totalLexerTime = totalLexerTime + dur
    return tokens
}